
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>Supported classifiers</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/gallery.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '2.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="None" href="index.html" />
    <link rel="next" title="Developer’s guide" href="developers_guide.html" />
    <link rel="prev" title="Feature JSON file format" href="features.html" />
  
   
       <script type="text/javascript" src="_static/sidebar.js"></script>
   
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/classifiers.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    var bodywrapper = $('.bodywrapper');
    var sidebarbutton = $('#sidebarbutton');
    sidebarbutton.css({'height': '900px'});
  </script>

  </head>
  <body role="document">

<div class="header-wrapper">
    <div class="header"></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/odeskdataproducts/cloudml">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel rellarge">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="features.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        Feature JSON fil...
        </span>
            <span class="hiddenrellink">
            Feature JSON file format
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="developers_guide.html"
        accesskey="N">Next
        <br/>
        <span class="smallrellink">
        Developer’s guid...
        </span>
            <span class="hiddenrellink">
            Developer’s guide
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
    </div>
    

<!--
      <p class="doc-version">This documentation is for Cloudml <strong>version 2.0</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p> -->
<!--     <p class="citing">If you use the software, please consider <a href="about.html#citing-scikit-learn">citing scikit-learn</a>.</p> -->
    <ul>
<li><a class="reference internal" href="#">Supported classifiers</a><ul>
<li><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li><a class="reference internal" href="#stochastic-gradient-descent-classifier">Stochastic Gradient Descent Classifier</a></li>
<li><a class="reference internal" href="#support-vector-regression">Support Vector Regression</a></li>
<li><a class="reference internal" href="#decision-tree">Decision Tree</a></li>
<li><a class="reference internal" href="#extra-tree">Extra Tree</a></li>
<li><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li><a class="reference internal" href="#gradient-boosting">Gradient Boosting</a></li>
</ul>
</li>
</ul>

    </div>
</div>



      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="supported-classifiers">
<h1><a class="toc-backref" href="#id131">Supported classifiers</a><a class="headerlink" href="#supported-classifiers" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#supported-classifiers" id="id131">Supported classifiers</a><ul>
<li><a class="reference internal" href="#logistic-regression" id="id132">Logistic Regression</a></li>
<li><a class="reference internal" href="#stochastic-gradient-descent-classifier" id="id133">Stochastic Gradient Descent Classifier</a></li>
<li><a class="reference internal" href="#support-vector-regression" id="id134">Support Vector Regression</a></li>
<li><a class="reference internal" href="#decision-tree" id="id135">Decision Tree</a></li>
<li><a class="reference internal" href="#extra-tree" id="id136">Extra Tree</a></li>
<li><a class="reference internal" href="#random-forest" id="id137">Random Forest</a></li>
<li><a class="reference internal" href="#gradient-boosting" id="id138">Gradient Boosting</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="logistic-regression">
<span id="classifier-logistic-regression"></span><h2><a class="toc-backref" href="#id132">Logistic Regression</a><a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">Scikit Learn LogisticRegression</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;logistic regression&quot;,
         &quot;params&quot;: {&quot;penalty&quot;: &quot;l2&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><cite>penalty</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, {l1, l2}</span></dt>
<dd><p class="first last">Used to specify the norm used in the penalization. The newton-cg andlbfgs solvers support only l2 penalties.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>dual</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Dual or primal formulation. Dual formulation is only implemented forl2 penalty with liblinear solver. Prefer dual=False whenn_samples &gt; n_features.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>C</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd><p class="first last">Inverse of regularization strength; must be a positive float.Like in support vector machines, smaller values specify strongerregularization.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>fit_intercept</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd><p class="first last">Specifies if a constant (a.k.a. bias or intercept) should beadded the decision function.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>intercept_scaling</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd><p class="first last">Useful only if solver is liblinear.when self.fit_intercept is True, instance vector x becomes[x, self.intercept_scaling],i.e. a &#8220;synthetic&#8221; feature with constant value equals tointercept_scaling is appended to the instance vector.The intercept becomes intercept_scaling * synthetic feature weightNote! the synthetic feature weight is subject to l1/l2 regularizationas all other features.To lessen the effect of regularization on synthetic feature weight(and therefore on the intercept) intercept_scaling has to be increased.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>class_weight</cite> <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;auto&#8217; or a dictionary</span></dt>
<dd><p class="first last">Over-/undersamples the samples of each class according to the givenweights. If not given, all classes are supposed to have weight one.The &#8216;auto&#8217; mode selects weights inversely proportional to classfrequencies in the training set.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>max_iter</cite> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">Useful only for the newton-cg and lbfgs solvers. Maximum number ofiterations taken for the solvers to converge.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>random_state</cite> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">The seed of the pseudo random number generator to use whenshuffling the data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>solver</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, {newton-cg, lbfgs, liblinear}</span></dt>
<dd><p class="first last">Algorithm to use in the optimization problem.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>tol</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">Tolerance for stopping criteria.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>multi_class</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, {ovr, multinomial}</span></dt>
<dd><p class="first last">Multiclass option can be either &#8216;ovr&#8217; or &#8216;multinomial&#8217;. If the optionchosen is &#8216;ovr&#8217;, then a binary problem is fit for each label. Elsethe loss minimised is the multinomial loss fit acrossthe entire probability distribution. Works only for the &#8216;lbfgs&#8217;solver.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>verbose</cite> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">For the liblinear and lbfgs solvers set verbose to any positivenumber for verbosity.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="stochastic-gradient-descent-classifier">
<span id="classifier-stochastic-gradient-descent-classifier"></span><h2><a class="toc-backref" href="#id133">Stochastic Gradient Descent Classifier</a><a class="headerlink" href="#stochastic-gradient-descent-classifier" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn-linear-model-sgdclassifier">Scikit Learn SGDClassifier</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;stochastic gradient descent classifier&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><cite>loss</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, {hinge, log, modified_huber, squared_hinge, perceptron, squared_loss, huber, epsilon_insensitive, squared_epsilon_insensitive}</span></dt>
<dd><p class="first last">The loss function to be used. Defaults to &#8216;hinge&#8217;, which gives alinear SVM.The &#8216;log&#8217; loss gives logistic regression, a probabilistic classifier.&#8217;modified_huber&#8217; is another smooth loss that brings tolerance tooutliers as well as probability estimates.&#8217;squared_hinge&#8217; is like hinge but is quadratically penalized.&#8217;perceptron&#8217; is the linear loss used by the perceptron algorithm.The other losses are designed for regression but can be useful inclassification as well; see SGDRegressor for a description.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>penalty</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, {none, l2, l1, elasticnet}</span></dt>
<dd><p class="first last">The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and&#8217;elasticnet&#8217; might bring sparsity to the model (feature selection)not achievable with &#8216;l2&#8217;.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>alpha</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">Constant that multiplies the regularization term. Defaults to 0.0001</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>l1_ratio</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.Defaults to 0.15.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>fit_intercept</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered. Defaults to True.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>n_iter</cite> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">The number of passes over the training data (aka epochs). The numberof iterations is set to 1 if using partial_fit.Defaults to 5.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>shuffle</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Whether or not the training data should be shuffled after each epoch.Defaults to False.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>random_state</cite> <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">The seed of the pseudo random number generator to use whenshuffling the data.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>verbose</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">The verbosity level</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>epsilon</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is&#8217;huber&#8217;, &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;.For &#8216;huber&#8217;, determines the threshold at which it becomes lessimportant to get the prediction exactly right.For epsilon-insensitive, any differences between the current predictionand the correct label are ignored if they are less than this threshold.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>n_jobs</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">The number of CPUs to use to do the OVA (One Versus All, formulti-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaultsto 1.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>learning_rate</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">The learning rate schedule:constant: eta = eta0optimal: eta = 1.0 / (t + t0) [default]invscaling: eta = eta0 / pow(t, power_t)where t0 is chosen by a heuristic proposed by Leon Bottou.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>eta0</cite> <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd><p class="first last">The initial learning rate for the &#8216;constant&#8217; or &#8216;invscaling&#8217;schedules. The default value is 0.0 as eta0 is not used by thedefault schedule &#8216;optimal&#8217;.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>power_t</cite> <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd><p class="first last">The exponent for inverse scaling learning rate [default 0.5].</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>class_weight</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">Preset for the class_weight fit parameter.Weights associated with classes. If not given, all classesare supposed to have weight one.The &#8220;auto&#8221; mode uses the values of y to automatically adjustweights inversely proportional to class frequencies.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>warm_start</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>average</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">When set to True, computes the averaged SGD weights and stores theresult in the <a href="#id139"><span class="problematic" id="id140">coef_</span></a> attribute. If set to an int greater than 1,averaging will begin once the total number of samples seen reachesaverage. So average=10 will begin averaging after seeing 10 samples.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="support-vector-regression">
<span id="classifier-support-vector-regression"></span><h2><a class="toc-backref" href="#id134">Support Vector Regression</a><a class="headerlink" href="#support-vector-regression" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn-svm-svr">Scikit Learn SVR</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;support vector regression&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><a href="#id1"><span class="problematic" id="id2">`</span></a>C ` <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd><p class="first last">penalty parameter C of the error term.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id3"><span class="problematic" id="id4">`</span></a>epsilon ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">epsilon in the epsilon-SVR model. It specifies the epsilon-tubewithin which no penalty is associated in the training loss functionwith points predicted within a distance epsilon from the actualvalue.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id5"><span class="problematic" id="id6">`</span></a>kernel ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8217;rbf&#8217;</span></dt>
<dd><p class="first last">Specifies the kernel type to be used in the algorithm.It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; ora callable.If none is given, &#8216;rbf&#8217; will be used. If a callable is given it isused to precompute the kernel matrix.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id7"><span class="problematic" id="id8">`</span></a>degree ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=3</span></dt>
<dd><p class="first last">degree of kernel functionis significant only in poly, rbf, sigmoid</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id9"><span class="problematic" id="id10">`</span></a>gamma ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n_featureswill be taken.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id11"><span class="problematic" id="id12">`</span></a>coef0 ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">independent term in kernel function. It is only significantin poly/sigmoid.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><cite>shrinking</cite> <span class="classifier-delimiter">:</span> <span class="classifier">string, default=True</span></dt>
<dd><p class="first last">Whether to use the shrinking heuristic.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id13"><span class="problematic" id="id14">`</span></a>tol ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">Tolerance for stopping criterion.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id15"><span class="problematic" id="id16">`</span></a>cache_size ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">Specify the size of the kernel cache (in MB)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id17"><span class="problematic" id="id18">`</span></a>verbose ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id19"><span class="problematic" id="id20">`</span></a>max_iter ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=-1</span></dt>
<dd><p class="first last">Hard limit on iterations within solver, or -1 for no limit.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="decision-tree">
<span id="id21"></span><h2><a class="toc-backref" href="#id135">Decision Tree</a><a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">Scikit Learn Decision Tree Classifier</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;decision tree classifier&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><a href="#id22"><span class="problematic" id="id23">`</span></a>criterion ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8221;gini&#8221;</span></dt>
<dd><p class="first last">The function to measure the quality of a split. Supported criteria are&#8221;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id24"><span class="problematic" id="id25">`</span></a>splitter ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8221;best&#8221;</span></dt>
<dd><p class="first last">The strategy used to choose the split at each node. Supportedstrategies are &#8220;best&#8221; to choose the best split and &#8220;random&#8221; to choosethe best random split.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id26"><span class="problematic" id="id27">`</span></a>max_features ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=None</span></dt>
<dd><p class="first last">The number of features to consider when looking for the best split:- If int, then consider <cite>max_features</cite> features at each split.- If float, then <cite>max_features</cite> is a percentage and`int(max_features * n_features)` features are considered at eachsplit.- If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.- If None, then <cite>max_features=n_features</cite>.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id28"><span class="problematic" id="id29">`</span></a>max_depth ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id30"><span class="problematic" id="id31">`</span></a>min_samples_split ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=2</span></dt>
<dd><p class="first last">The minimum number of samples required to split an internal node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id32"><span class="problematic" id="id33">`</span></a>min_samples_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=1</span></dt>
<dd><p class="first last">The minimum number of samples required to be at a leaf node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id34"><span class="problematic" id="id35">`</span></a>min_weight_fraction_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">The minimum weighted fraction of the input samples required to be at aleaf node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id36"><span class="problematic" id="id37">`</span></a>max_leaf_nodes ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id38"><span class="problematic" id="id39">`</span></a>class_weight ` <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">(default=None)Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.If not given, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.The &#8220;auto&#8221; mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input data.For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id40"><span class="problematic" id="id41">`</span></a>random_state ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=None</span></dt>
<dd><p class="first last">If int, random_state is the seed used by the random number generator;If RandomState instance, random_state is the random number generator;If None, the random number generator is the RandomState instance usedby <cite>np.random</cite>.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="extra-tree">
<span id="id42"></span><h2><a class="toc-backref" href="#id136">Extra Tree</a><a class="headerlink" href="#extra-tree" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">Scikit Learn ExtraTreesClassifier</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;extra trees classifier&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><a href="#id43"><span class="problematic" id="id44">`</span></a>n_estimators ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=10</span></dt>
<dd><p class="first last">The number of trees in the forest.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id45"><span class="problematic" id="id46">`</span></a>criterion ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8221;gini&#8221;</span></dt>
<dd><p class="first last">The function to measure the quality of a split. Supported criteria are&#8221;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id47"><span class="problematic" id="id48">`</span></a>max_features ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=&#8221;auto&#8221;</span></dt>
<dd><p class="first last">The number of features to consider when looking for the best split:- If int, then consider <cite>max_features</cite> features at each split.- If float, then <cite>max_features</cite> is a percentage and`int(max_features * n_features)` features are considered at eachsplit.- If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.- If None, then <cite>max_features=n_features</cite>.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id49"><span class="problematic" id="id50">`</span></a>max_depth ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id51"><span class="problematic" id="id52">`</span></a>min_samples_split ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=2</span></dt>
<dd><p class="first last">The minimum number of samples required to split an internal node.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id53"><span class="problematic" id="id54">`</span></a>min_samples_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=1</span></dt>
<dd><p class="first last">The minimum number of samples in newly created leaves.  A split isdiscarded if after the split, one of the leaves would contain less then``min_samples_leaf`` samples.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id55"><span class="problematic" id="id56">`</span></a>min_weight_fraction_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">The minimum weighted fraction of the input samples required to be at aleaf node.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id57"><span class="problematic" id="id58">`</span></a>max_leaf_nodes ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id59"><span class="problematic" id="id60">`</span></a>bootstrap ` <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">Whether bootstrap samples are used when building trees.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id61"><span class="problematic" id="id62">`</span></a>oob_score ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Whether to use out-of-bag samples to estimatethe generalization error.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id63"><span class="problematic" id="id64">`</span></a>n_jobs ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=1</span></dt>
<dd><p class="first last">The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.If -1, then the number of jobs is set to the number of cores.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id65"><span class="problematic" id="id66">`</span></a>random_state ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=None</span></dt>
<dd><p class="first last">If int, random_state is the seed used by the random number generator;If RandomState instance, random_state is the random number generator;If None, the random number generator is the RandomState instance usedby <cite>np.random</cite>.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id67"><span class="problematic" id="id68">`</span></a>verbose ` <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">Controls the verbosity of the tree building process.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id69"><span class="problematic" id="id70">`</span></a>warm_start ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id71"><span class="problematic" id="id72">`</span></a>class_weight ` <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.If not given, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.The &#8220;auto&#8221; mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input data.The &#8220;subsample&#8221; mode is the same as &#8220;auto&#8221; except that weights arecomputed based on the bootstrap sample for every tree grown.For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="random-forest">
<span id="id73"></span><h2><a class="toc-backref" href="#id137">Random Forest</a><a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Scikit Learn RandomForestClassifier</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;random forest classifier&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><a href="#id74"><span class="problematic" id="id75">`</span></a>n_estimators ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=10</span></dt>
<dd><p class="first last">The number of trees in the forest.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id76"><span class="problematic" id="id77">`</span></a>criterion ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8221;gini&#8221;</span></dt>
<dd><p class="first last">The function to measure the quality of a split. Supported criteria are&#8221;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id78"><span class="problematic" id="id79">`</span></a>max_features ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=&#8221;auto&#8221;</span></dt>
<dd><p class="first last">The number of features to consider when looking for the best split:- If int, then consider <cite>max_features</cite> features at each split.- If float, then <cite>max_features</cite> is a percentage and`int(max_features * n_features)` features are considered at eachsplit.- If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.- If None, then <cite>max_features=n_features</cite>.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id80"><span class="problematic" id="id81">`</span></a>max_depth ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id82"><span class="problematic" id="id83">`</span></a>min_samples_split ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=2</span></dt>
<dd><p class="first last">The minimum number of samples required to split an internal node.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id84"><span class="problematic" id="id85">`</span></a>min_samples_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=1</span></dt>
<dd><p class="first last">The minimum number of samples in newly created leaves.  A split isdiscarded if after the split, one of the leaves would contain less then``min_samples_leaf`` samples.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id86"><span class="problematic" id="id87">`</span></a>min_weight_fraction_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">The minimum weighted fraction of the input samples required to be at aleaf node.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id88"><span class="problematic" id="id89">`</span></a>max_leaf_nodes ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.Note: this parameter is tree-specific.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id90"><span class="problematic" id="id91">`</span></a>bootstrap ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=True</span></dt>
<dd><p class="first last">Whether bootstrap samples are used when building trees.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id92"><span class="problematic" id="id93">`</span></a>oob_score ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">Whether to use out-of-bag samples to estimatethe generalization error.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id94"><span class="problematic" id="id95">`</span></a>n_jobs ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=1</span></dt>
<dd><p class="first last">The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.If -1, then the number of jobs is set to the number of cores.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id96"><span class="problematic" id="id97">`</span></a>random_state ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=None</span></dt>
<dd><p class="first last">If int, random_state is the seed used by the random number generator;If RandomState instance, random_state is the random number generator;If None, the random number generator is the RandomState instance usedby <cite>np.random</cite>.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id98"><span class="problematic" id="id99">`</span></a>verbose ` <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">Controls the verbosity of the tree building process.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id100"><span class="problematic" id="id101">`</span></a>warm_start ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id102"><span class="problematic" id="id103">`</span></a>class_weight ` <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.If not given, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.The &#8220;auto&#8221; mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input data.The &#8220;subsample&#8221; mode is the same as &#8220;auto&#8221; except that weights arecomputed based on the bootstrap sample for every tree grown.For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="gradient-boosting">
<span id="id104"></span><h2><a class="toc-backref" href="#id138">Gradient Boosting</a><a class="headerlink" href="#gradient-boosting" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">Scikit Learn GradientBoostingClassifier</a> will be used as the underlying implementation.</p>
<p>Sample configuration in features.json file:</p>
<div class="highlight-json"><div class="highlight"><pre>&quot;classifier&quot;: {
         &quot;type&quot;: &quot;gradient boosting classifier&quot;,
         &quot;params&quot;: {&quot;loss&quot;: &quot;log&quot;}
}
</pre></div>
</div>
<p>This classifier has following parameters:</p>
<ul>
<li><dl class="first docutils">
<dt><a href="#id105"><span class="problematic" id="id106">`</span></a>loss ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8217;deviance&#8217;</span></dt>
<dd><p class="first last">loss function to be optimized. &#8216;deviance&#8217; refers todeviance (= logistic regression) for classificationwith probabilistic outputs. For loss &#8216;exponential&#8217; gradientboosting recoveres the AdaBoost algorithm.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id107"><span class="problematic" id="id108">`</span></a>learning_rate ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.There is a trade-off between learning_rate and n_estimators.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id109"><span class="problematic" id="id110">`</span></a>n_estimators ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=100, {int ()}</span></dt>
<dd><p class="first last">The number of boosting stages to perform. Gradient boostingis fairly robust to over-fitting so a large number usuallyresults in better performance.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id111"><span class="problematic" id="id112">`</span></a>max_depth ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=3</span></dt>
<dd><p class="first last">maximum depth of the individual regression estimators. The maximumdepth limits the number of nodes in the tree. Tune this parameterfor best performance; the best value depends on the interactionof the input variables.Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id113"><span class="problematic" id="id114">`</span></a>min_samples_split ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=2</span></dt>
<dd><p class="first last">The minimum number of samples required to split an internal node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id115"><span class="problematic" id="id116">`</span></a>min_samples_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=1</span></dt>
<dd><p class="first last">The minimum number of samples required to be at a leaf node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id117"><span class="problematic" id="id118">`</span></a>min_weight_fraction_leaf ` <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd><p class="first last">The minimum weighted fraction of the input samples required to be at aleaf node.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id119"><span class="problematic" id="id120">`</span></a>subsample ` <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span></dt>
<dd><p class="first last">The fraction of samples to be used for fitting the individual baselearners. If smaller than 1.0 this results in Stochastic GradientBoosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of varianceand an increase in bias.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id121"><span class="problematic" id="id122">`</span></a>max_features ` <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=None</span></dt>
<dd><p class="first last">The number of features to consider when looking for the best split:- If int, then consider <cite>max_features</cite> features at each split.- If float, then <cite>max_features</cite> is a percentage and`int(max_features * n_features)` features are considered at eachsplit.- If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.- If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.- If None, then <cite>max_features=n_features</cite>.Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of varianceand an increase in bias.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id123"><span class="problematic" id="id124">`</span></a>max_leaf_nodes ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id125"><span class="problematic" id="id126">`</span></a>init ` <span class="classifier-delimiter">:</span> <span class="classifier">string, default=None</span></dt>
<dd><p class="first last">An estimator object that is used to compute the initialpredictions. <code class="docutils literal"><span class="pre">init</span></code> has to provide <code class="docutils literal"><span class="pre">fit</span></code> and <code class="docutils literal"><span class="pre">predict</span></code>.If None it uses <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id127"><span class="problematic" id="id128">`</span></a>verbose ` <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd><p class="first last">Enable verbose output. If 1 then it prints progress and performanceonce in a while (the more trees the lower the frequency). If greaterthan 1 then it prints progress and performance for every tree.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><a href="#id129"><span class="problematic" id="id130">`</span></a>warm_start ` <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd><p class="first last">When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just erase theprevious solution.</p>
</dd>
</dl>
</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2015, Upwork.
      <a href="_sources/classifiers.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel rellarge">
    
    <div class="buttonPrevious">
      <a href="features.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="developers_guide.html">Next
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>